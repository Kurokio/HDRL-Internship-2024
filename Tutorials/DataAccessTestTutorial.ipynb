{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8356143-f305-4192-875a-0e915df681f9",
   "metadata": {},
   "source": [
    "# Data Access Check Tutorial\n",
    "This notebook shows how to use the DataAccessCheck script to test all the HAPI links in the database for data accessibility. This script was not tested on non-HAPI data access links.\n",
    "\n",
    "*Note that if you want to run for all 1632 HAPI links present in the database, expect the script to run for a LONG time. It is recommended to only do so if you are fine waiting overnight or even for an entire day or more.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bad10d-b7cd-4708-90c2-ed46afbe1762",
   "metadata": {},
   "source": [
    "## Select the amount and type of datasets you wish to test.\n",
    "\n",
    "Note that you may change the database file that is loaded in the call to create_sqlite_database. This is especially important if you created your own from scratch in the HowToUse_Advanced tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f062839a-b90f-4570-8d45-f98c398b9a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataAccessCheck import DataChecker\n",
    "from Scripts.SQLiteFun import create_sqlite_database, execution\n",
    "from contextlib import closing, redirect_stdout\n",
    "from IPython.utils.io import Tee\n",
    "\n",
    "# input abs path of database file you wish to query from\n",
    "conn = create_sqlite_database(\"/home/jovyan/HDRL-Internship-2024/SPASE_Data_20240716.db\")\n",
    "# create list to hold all dataset names acquired from db\n",
    "prodKeys = [] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f914ec1-e037-4a5c-95f0-1603b637e08b",
   "metadata": {},
   "source": [
    "The same functionality to sqlite statements shown in the HowToUse notebooks still applies. For instance, you can tailor the datasets you wish to test however you like, such as by mission, author, publisher, publication year, etc. This, of course, is done by adding additional WHERE arguments.\n",
    "\n",
    "A basic example query would be to select the first 10 HAPI datasets stored in the database, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5ea345-afe0-496e-ad8b-12aa3001d359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if want to run on ALL 1632 datasets, remove the 'LIMIT 10' text from the query\n",
    "HapiStmt = \"\"\"SELECT prodKey FROM MetadataEntries WHERE url LIKE '%/hapi' LIMIT 10\"\"\"\n",
    "prodKeys = execution(HapiStmt, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d05ca3-bf53-4490-a373-40a137de4bfb",
   "metadata": {},
   "source": [
    "Note that you can test the nth dataset(s) by offsetting where the query starts in the database. For example, if you wish to test the datasets that are in positions 60-70 in the database, you would perform the following query instead.\n",
    "\n",
    "``` python\n",
    "HapiStmt = \"\"\"SELECT prodKey FROM MetadataEntries WHERE url LIKE '%/hapi' LIMIT 10 OFFSET 59\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd4b40e-4b8e-4a7e-bb50-200abcc6f903",
   "metadata": {},
   "source": [
    "## Execute the script and wait for the results!\n",
    "\n",
    "The following code blocks test the datasets specified from your above query, with options to print the outputs to both the console and a text file or instead to only a text file.\n",
    "\n",
    "*If testing all datasets in the database, it is advised to only output to a text file.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1ed8f6-73f3-4cf6-8c27-7c6fbb7d3aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(DataChecker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb29bae-6ae6-4f5f-9426-9c4ff12e2a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if want output only in file\n",
    "with open(\"../DatalinkCheckOutputTest.txt\", \"w\") as file:\n",
    "    with redirect_stdout(file):\n",
    "        lines = DataChecker(prodKeys, conn)\n",
    "\n",
    "# if want both file and console\n",
    "#with closing(Tee(\"../DatalinkCheckOutputTest.txt\", \"w\", channel=\"stdout\")) as outputstream:\n",
    " #   lines = DataChecker(prodKeys, conn)\n",
    "print(\"The program is done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d5248f-8bbb-4a83-a4be-98df9775eebd",
   "metadata": {},
   "source": [
    "## Analyzing the results\n",
    "\n",
    "Before analyzing the results, the code creates another text file containing just those datasets that timed out at some or all time intervals and failed to retrieve data. Afterwards, the code prints out the results.\n",
    "\n",
    "> For a more detailed explanation of the results from the script, use the following key to query the TestResults table in the database, specifically the dataAccess and Errors columns.\n",
    "> Possible outcomes for each dataset and their corresponding message recorded in the database:\n",
    "> - Data successfully accessed --> \"Passed\" value in dataAccess\n",
    "\n",
    ">> - Data was successfully accessed BUT some intervals timed out --> Also gets \"Passed after some intervals timed out\" in Errors\n",
    "\n",
    "> - No data was accessed --> \"Failed\" in dataAccess and \"HAPI data check failed after _ attempts\" in Errors\n",
    "\n",
    ">> - No data was accessed but some intervals timed out --> \"Failed data check but some intervals timed out\" in Errors\n",
    "\n",
    ">> - Initial data info check failed --> \"HAPI info check failed\" in Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3d251a-1fd5-42f7-aa4c-0f32d65b775e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export all datasets which take too long to a text file for further investigation\n",
    "#   can use this subset of datasets as the prodKeys argument for another script iteration\n",
    "textFile = open(\"../HAPI_TakeTooLongTest.txt\", \"w\")\n",
    "for line in lines:\n",
    "    textFile.write(line)\n",
    "    textFile.write(\"\\n\")\n",
    "textFile.close()\n",
    "\n",
    "Datasets = []\n",
    "dataFails = 0\n",
    "tookTooLong = 0\n",
    "HAPIErrors = 0\n",
    "Unavailable = 0\n",
    "with open(\"../DatalinkCheckOutputTest.txt\") as file:\n",
    "    for line in file:\n",
    "        if (\"No data was found.\") in line:\n",
    "            dataFails += 1\n",
    "        elif \"Bad request - unknown dataset id\" in line:\n",
    "            Unavailable += 1\n",
    "        elif \"Problem with https://cdaweb.gsfc.nasa.gov/hapi/info?\" in line:\n",
    "            HAPIErrors += 1\n",
    "            before, sep, after = line.partition(\"Problem with https://cdaweb.gsfc.nasa.gov/hapi/info?id=\")\n",
    "            dataset, sep, after = after.partition(\".\")\n",
    "            Datasets.append(dataset)\n",
    "            \n",
    "with open(\"../HAPI_TakeTooLongTest.txt\") as file:\n",
    "    tookTooLong = len(file.readlines())\n",
    "\n",
    "dataSuccesses = len(prodKeys) - (tookTooLong + HAPIErrors + dataFails + Unavailable)\n",
    "print(\"The number of links that successfully retrieved data are \" + str(dataSuccesses))\n",
    "print(\"The number of broken links is \" + str(dataFails))\n",
    "print(\"The number of links that are not actual datasets in CDAWeb are \" + str(Unavailable))\n",
    "print(\"The number of links that encountered another HAPIError are \" + str(HAPIErrors))\n",
    "print(\"The number of links that timed out is \" + str(tookTooLong))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
